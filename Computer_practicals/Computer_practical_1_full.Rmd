---
title: 'Computer practical 1: Topics in Statistics III/IV, Term 1'
author: "Georgios Karagiannis"
output:
  html_notebook: default
  pdf_document: default
  word_document: default
  html_document: default
---

<!-- -------------------------------------------------------------------------------- -->

<!-- Copyright 2020 Georgios Karagiannis -->

<!-- This file is part of Topics_in_Statistics_Michaelmas_2020 -->
<!-- (Topics in Statistics III/IV (MATH3361/4071) Michaelmas term 2020) -->
<!-- which is the material of the course (Topics in Statistics III/IV (MATH3361/4071) -->
<!-- taught by Georgios P. Katagiannis in the Department of Mathematical Sciences   -->
<!-- in the University of Durham  in Michaelmas term in 2020 -->

<!-- Topics_in_Statistics_Michaelmas_2020 is free software: you can redistribute it and/or modify -->
<!-- it under the terms of the GNU General Public License as published by -->
<!-- the Free Software Foundation version 3 of the License. -->

<!-- Topics_in_Statistics_Michaelmas_2020 is distributed in the hope that it will be useful, -->
<!-- but WITHOUT ANY WARRANTY; without even the implied warranty of -->
<!-- MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the -->
<!-- GNU General Public License for more details. -->

<!-- You should have received a copy of the GNU General Public License -->
<!-- along with Topics_in_Statistics_Michaelmas_2020 If not, see <http://www.gnu.org/licenses/>. -->

<!-- -------------------------------------------------------------------------------- -->


> Aim
>
> * To become familiar with Iterative Proportional Fitting (IPF) method
> * Apply IPF method to produce MLE for the Log Linear models
> 
> * To learn how to solve systems of noon-linear equations via Newton method 
> * Apply Newton method to produce MLE for the Log Linear models
>
> * Extensions of Newton method, and IPF method to to produce MLEs for 4-way and higher order tables


---------

# Contigency table: data manipulation

Below we load a table where refers to a 1992 survey by the Wright State University School of Medicine and the United Health Services in Dayton, Ohio. The survey asked 2276 students in their final year of high school in a nonurban area near Dayton, Ohio whether they had ever used alcohol, cigarettes, or marijuana. Denote the variables in this $2 \times 2 \times 2$ table by A for alcohol use, C for cigarette use, and M for marijuana use.

> Load the observed counts in a data frame ` obs.frame  ` and print the result. Use commands :
> 
> * `data.frame()` : as in SC2
> * `factor()` :  to encode a vector as a factor (aka category)
> * `expand.grid()` : to produce all combinations of the supplied vectors or factors.


```{r}

# I will do this for you

## load the data

obs.frame<-data.frame(count=c(911,538,44,456,3,43,2,279),
                      expand.grid( 
  marijuana=factor(c("Yes","No"),levels=c("No","Yes")),
  cigarette=factor(c("Yes","No"),levels=c("No","Yes")),  
  alcohol=factor(c("Yes","No"),levels=c("No","Yes")))
  ) 

## orint the obs.frame

obs.frame

```

> Create $3$ dimentional contingency table from `obs.frame`. Use command:
> 
> * `xtabs()`, to create a contingency table from cross-classifying factors in a dara.frame

```{r}
# this is me again
obs.xtabs <- xtabs(count ~ marijuana+cigarette+alcohol, data=obs.frame)
## print 
obs.xtabs
```

> Create a contigency table which contains the row, column, layer, etc...margins. 
> 
> *  Use command `addmargins()` with `obs.xtabs`
> 
> * Save it in `obs.addmargins`

```{r}
obs.addmargins <- addmargins(obs.xtabs)
obs.addmargins
```

> Compute the marginal contigency table of marijuana and cigarette. 
> 
> *  Use command `margin.table( , margin = )` and `obs.xtabs`. 
> 
> *  Save it in `obs.mc.xtabs`.

```{r}
obs.xtabs <- xtabs(count ~ marijuana+cigarette+alcohol, data=obs.frame)
obs.mc.xtabs <- margin.table(obs.xtabs, margin=c(1,2))
obs.mc.xtabs
```

> Compute the (joint) sampling proportions. 
> 
> *  Use command  `prop.table()` with `obs.xtabs`. 
> 
> *  Save it in `obs.prop.table`.

```{r}
obs.xtabs <- xtabs(count ~ marijuana+cigarette+alcohol, data=obs.frame)
obs.prop.table <- prop.table(obs.xtabs)
obs.prop.table
```

> Create a data.frame of proportions. 
> 
> *  Use the command `as.data.frame()` with `obs.prop.table` 
> 
> * Save it as `obs.prop.frame` 

```{r}
obs.prop.frame <- as.data.frame(obs.prop.table)
```


> As a homework, you can furhter experiment with commands   
> 
> * `margin.table` : computing  marginal tables  
> 
> * `prop.table`   : computing proportions  
> 
> * `addmargins`   : putting margins on tables;  
> 
>     + e.g., obs.prop.margin <- addmargins(prop.table(obs.xtabs))  


```{r}
#
# Do it later ...
#
```


---

# Odds ratio calculations

> Code an R function, named 'odds.ratio' with:  
>
> * Inputs:  
>   * x : a 2 by 2 matrix whose elements are the observed counts of a 2 by 2 contigency table  
>   * conf.level : with default input value 0.95 representing the confidence level  
>   * theta0 : with default value 1 representing a null hypothesis value of the odds ratio test  
>
> * Outputs:  
>   * estimator : representing mle of odds satio  
>   * log.estimator : representing the mle of log odds ratio
>   * asympt.SE : representing the standard error / standard deviation of the mle of odds ratio  
>   * conf.interval: representing  confidence interval of mle of odds ratio at sig level conf.level (from the inputs)
>   * conf.level =representing  confidence level 
>   * Ztest : representing the test statistic for the odds ratio test (2 tails) 
>   * p.value : representing the p value of the odds ratio test (2 tails) 
>   * log.conf.interval : representing in log scale the confidence interval of mle of odds ratio at sig level conf.level (from the inputs)


```{r}
odds.ratio <- function(x,conf.level=0.95,theta0=1)
{
  if (any(x==0)) x <- x+0.5
  theta <- x[1,1] *
    x[2,2]/(x[1,2]  *
              x[2,1])
  SE <- sqrt(sum(1/x))
  Za2 <- qnorm(0.5 *
                 (1+conf.level))
  Low <- exp(log(theta)-Za2 * SE)
  Up <- exp(log(theta)+Za2  *
              SE)
  CI <- c(Low,Up)
  Z=(log(theta)-log(theta0))/SE
  pv=2   *
    pnorm(-abs(Z))
  
  logCI <- log(CI)
  
  list (estimator=theta, 
        log.estimator=log(theta), 
        asympt.SE=SE, 
        conf.interval=CI,
        conf.level=conf.level, 
        Ztest=Z, 
        p.value=pv,
        log.conf.interval=logCI
        )
}
```



> For the  marginal contigency table of marijuana and cigarette, 
> * compute the mle of the marginal odds ratio of marijuana and cigarette
> * computet the 95% Confidence Interval of the marginal odds ratio of marijuana and cigarette
> * perform a statiastical hypothesis test that marijuana and cigarette are independent at sig level 0.05 

```{r}
obs.xtabs <- xtabs(count ~ marijuana+cigarette+alcohol, data=obs.frame)

obs.mc.xtabs <- margin.table(obs.xtabs, margin=c(1,2))

odds.ratio.marijuana.cigarette <- odds.ratio(obs.mc.xtabs, conf.level=0.95, theta0=1 )
odds.ratio.marijuana.cigarette

```
The MLE of the marginal odds ratio of marijuana and cigarette  is `r  odds.ratio.marijuana.cigarette$estimator` .  

The 95% confidence interval of the marginal odds ratio of marijuana and cigarette is [`r odds.ratio.marijuana.cigarette$conf.interval`] .

The hypothesis test with $H_0:\theta=1$ versus $H_1:\theta\ne1$ at sig. level 0.05 has a p-value `r odds.ratio.marijuana.cigarette$p.value` Hence I reject $H_0:\theta=1$ against $H_1:\theta\ne1$ at sig. level 0.05. 


---

## Fourfold Plots

You can draw Fourfold Plots


### Tables 2 x 2

It is a graphical expression visualizing the odds ratio  

$$\theta = \frac{n_{11}n_{22}}{n_{12}n_{21}}$$

in 2 x 2 contingency tables. 

It shows the departure from independence as  measured by the sample odds ratio, 

Each cell $n_{ij}$ is represented as a quarter-circle with radius proportional to $\sqrt{n_{ij}}$ and area proportional to $n_{ij}$.  

* If there is no association $\theta=1$ between classification variables, the quarter-circles should form a circle.

* If there is positive association $\theta>1$ between classification variables, the  diagonal areas are greater  than the off-diagonal areas

* If there is negative association $\theta<1$ between classification variables, the  diagonal areas are smaller  than the off-diagonal areas

R provides a function to draw this kind of plots  by using the function `fourfoldplot' from the package `vcd' 

> * Install `vcd' package and load it 

```{r}
# install.packages('vcd') # uncomment it if you have not installed package vcd
library(vcd)
```

> Check in help the function fourfoldplot by usign the command `?fourfoldplot'
> 
> * Draw a Fourfold Plot for the marginal contigency table of marijuana and cigarette.  
> * Discuss what you can see  

```{r}
obs.mc.xtabs <- margin.table(obs.xtabs, margin=c(1,2))

fourfoldplot(obs.mc.xtabs)
```

Note that:  
* The area of each shaded quadrant shows the observed counts.  
* Circular arcs show the limits of confidence interval for the odds ratio.  


### Tables 2 x 2 x K

Fourfold Plots can be also used for 2 x 2 x K contigancy tables 

> * Draw a Fourfold Plot for the contigency table of marijuana cigarette, and alcohol by controlling on the alcohol levels.  
> * inspect the plots

```{r}
obs.xtabs <- xtabs(count ~ marijuana+cigarette+alcohol, data=obs.frame)

fourfoldplot(obs.xtabs,
             mfrow = c(1,2)
             )
```

## Mosaic plot 

Mosaic plot display graphically the cells of a contingency table as rectangular areas of size proportional to the corresponding observed frequencies.

When the classification variables are independent the areas tend to be perfectly aligned in rows and columns.  

The greater the deviation is, the worse the aforesaid alignment is.

Furthermore, specific locations of the table that deviate from independence the most can be identified and thus the pattern of underlying association can be explained. 

The strength of individual cells contribution to divergence from independence as well as the direction of the divergence are reflected in the magnitude and sign of the corresponding independence model’s residuals that can be incorporated in a mosaic plot.


R provides a function to draw this kind of plots  by using the function `mosaic' from the package `vcd' 

> * Install `vcd' package and load it 

```{r}
#install.packages('vcd') # uncomment it if you have not installed package vcd
library(vcd)
```

> * Check the command `mosaic' in help by typing `?mosaic' 

> For the I x J  case:
> * Draw a Mosaic Plot for the marginal contigency table of marijuana cigarette. 
>   * in particular use use mosaic(x,residuals_type="deviance",gp=shading_hcl)  where x is the contigency table of interest  
> * Interpretet the plots 


```{r}
mosaic(obs.mc.xtabs, 
       residuals_type="deviance",
       gp=shading_hcl)
```
* I observe a substantial deviation from the Independent association. 

* The p-value of the GoF test based on the deviance for independence model is below the colorbar, and smaller than 0.05; hence I reject the hipothesis at sig. level 5%.


> For the I x J x K  case:
> * Draw a Mosaic Plot for the contigency table of marijuana cigarette and alcohol. 
> * Interpretet the plots 

```{r}
obs.xtabs <- xtabs(count ~ marijuana+cigarette+alcohol, data=obs.frame)
mosaic(obs.xtabs, 
       residuals_type="deviance",
       gp=shading_hcl)
```

* I observe a substantial deviation from the Independent association. 

* The p-value of the GoF test based on the deviance for independence model is below the colorbar, and smaller than 0.05; hence I reject the hypothesis at sig. level 5%.



---------

# The Iterative Proportions Fitting method 

The iterative proportional fitting (IPF) algorithm is a simple method for calculating $\mu_{ijk}$ for log-linear models. 


The main idea of the procedure is the following:

*  Start with $\mu_{ijk...}^{(0)}$ satisfying a model no more complex
than the one being fitted. E.g., $\mu_{ijk...}^{(0)}=1.0$ should
be ok.

* For $t=1,...$, 

    + adjust $\mu_{ijk}^{(t)}$ to match by multiplying each marginal table
in the set of minimal sufficient statistics, by appropriate factors
    
    + escape the loop, when the maximum difference between the sufficient
statistics and their fitted values is sufficiently close to zero.



***Illustration:***

Consider $3$-way, $I\times J\times K$ tables, and with classifiers
$X$, $Y$,$Z$. 

Given the model $(XY,XZ,YZ)$ design a IPF recursion producing estimates for $\mu_{ijk}$'s 

Steps:

* Compute the minimal sufficient statistics are $\{n_{ij+}\}$, $\{n_{i+k}\}$, $\{n_{+jk}\}$.

* Assume that the approximated $\mu_{ijk}$'s from the $(t-1)$-th cycle is $\mu_{ijk}^{(t-1)}$. 

* Then the $t$-th cycle of the IPF algorithm has the following steps: 

    + Set $m_{ijk}^{(0)}=\mu_{ijk}^{(t-1)}$ 
    
    +  Compute 
\begin{align*}
m_{ijk}^{(1)} & =m_{ijk}^{(0)}\frac{n_{ij+}}{m_{ij+}^{(0)}};\;\forall i,j,k\\
m_{ijk}^{(2)} & =m_{ijk}^{(1)}\frac{n_{i+k}}{m_{i+k}^{(1)}};\;\forall i,j,k\\
m_{ijk}^{(3)} & =m_{ijk}^{(2)}\frac{n_{+jk}}{m_{+jk}^{(2)}};\;\forall i,j,k
\end{align*}

    + Set $\mu_{ijk}^{(t)}=m_{ijk}^{(3)}$, $\forall i,j,k$ 

...and produces $\mu_{ijk}^{(t)}$ as approximation.


### ***Example***

We use the Alcohol, Cigarette, and Marijuana   data-set

```{r}

# I will do this for you

## load the data

obs.frame<-data.frame(count=c(911,538,44,456,3,43,2,279),
                      expand.grid( 
  marijuana=factor(c("Yes","No"),levels=c("No","Yes")),
  cigarette=factor(c("Yes","No"),levels=c("No","Yes")),  
  alcohol=factor(c("Yes","No"),levels=c("No","Yes")))
  ) 

## orint the obs.frame

obs.frame

```

> * Consider the Log-linear model describing a homogeneous association between each pair of variables at each level of the third one; i.e. $[XY,XZ,YZ]$  
>   
> * Find the fitted values for $\mu_{i,j,k}$, by using your own code  
> * Check your results with R function 'loglin{stats}'  

***Solution***


```{r}

# Step 1 :  compute and save the minimal statistics

obs.xtab <- xtabs(obs.frame)
n_xy <- margin.table(obs.xtab,c(1,2))
n_xz <- margin.table(obs.xtab,c(1,3))
n_yz <- margin.table(obs.xtab,c(2,3))

# Step 2: Create a seed for the fitted mu's

# seed the mu_opt
mu_opt <- obs.xtab
for(i in 1:2)
  for(j in 1:2)
    for(k in 1:2)
      mu_opt[i,j,k] = 1.0

# Step 3: Perform the loop to approximate the mu's

for (t in 1: 100) {
  
  mu_xy <- margin.table(mu_opt,c(1,2))
for(i in 1:2)
  for(j in 1:2)
    for(k in 1:2) 
      mu_opt[i,j,k] <- mu_opt[i,j,k]*n_xy[i,j]/mu_xy[i,j]
  
  mu_xz <- margin.table(mu_opt,c(1,3))
  
for(i in 1:2)
  for(j in 1:2)
    for(k in 1:2) 
      mu_opt[i,j,k] <- mu_opt[i,j,k]*n_xz[i,k]/mu_xz[i,k]
  
  mu_yz <- margin.table(mu_opt,c(2,3))
for(i in 1:2)
  for(j in 1:2)
    for(k in 1:2) 
      mu_opt[i,j,k] <- mu_opt[i,j,k]*n_yz[j,k]/mu_yz[j,k]
}

mu_opt
```

```{r}
# CHECK WITH COMMAND loglin
obs.xtab <- xtabs(obs.frame)
loglin(obs.xtab,
         list(
           c(1,2),
           c(1,3),
           c(2,3)
           ),
       fit = TRUE)$fit
```



> Present the fitted $\mu$'s as a data frame

```{r}
as.data.frame(mu_opt)
```

> You can double check your result with the output of the R package

```{r}
obs.xtab <- xtabs(obs.frame)
library(MASS)
fitAC.AM.CM<-loglm( count~alcohol*cigarette+ alcohol*marijuana +cigarette*marijuana,
              data=obs.xtab,
              param=T,
              fit=T)
fit.array<- fitted(fitAC.AM.CM)
fit.array

as.data.frame(fit.array)
```


### ***Example***

> * Consider the Log-linear model describing mutual independence; i.e. $[X, Y,Z]$  
>   
> 1. Write your own code to compute the fitted values for $\mu_{i,j,k}$  
> 2. Check your ewsults with the R command 'loglin{stats}'  

***Solution***

```{r}

# Step 1 :  compute and save the minimal statistics

obs.xtab <- xtabs(obs.frame)
n_x <- margin.table(obs.xtab,c(1))
n_y <- margin.table(obs.xtab,c(2))
n_z <- margin.table(obs.xtab,c(3))

# Step 2: Create a seed for the fitted mu's

# seed the mu_opt
mu_opt <- obs.xtab
for(i in 1:2)
  for(j in 1:2)
    for(k in 1:2)
      mu_opt[i,j,k] = 1.0

# Step 3: Perform the loop to approximate the mu's

for (t in 1: 100) {
  
  mu_x <- margin.table(mu_opt,c(1))
  for(i in 1:2)
    for(j in 1:2)
      for(k in 1:2)
      mu_opt[i,j,k] <- mu_opt[i,j,k]*n_x[i]/mu_x[i]
  
  mu_y <- margin.table(mu_opt,c(2))
  for(i in 1:2)
    for(j in 1:2)
      for(k in 1:2)
      mu_opt[i,j,k] <- mu_opt[i,j,k]*n_y[j]/mu_y[j]
  
  mu_z <- margin.table(mu_opt,c(3))
  for(i in 1:2)
    for(j in 1:2)
      for(k in 1:2)
      mu_opt[i,j,k] <- mu_opt[i,j,k]*n_z[k]/mu_z[k]
}


mu_opt
```
```{r}
# CHECK WITH COMMAND loglin
obs.xtab <- xtabs(obs.frame)
loglin(obs.xtab,
         list(
           c(1),
           c(2),
           c(3)
           ),
       fit = TRUE)$fit
```


***Example (Homework)***

The table below, summarises summarizes observations of 68,694 passengers in autos and light
trucks involved in accidents in the state of Maine in 1991. The table classifies
passengers by gender (G), location of accident (Z), seat-belt use (S), and
injury (I). The Table reports the sample proportion of passengers who were
injured. For each GL combination, the proportion of injuries was about
halved for passengers wearing seat belts.

```{r}
# load dataset
obs.frame.accident<-data.frame(
  count=c(7287,11587,3246,6134,10381,10969,6123, 6693,996, 759, 973, 757, 812, 380, 1084, 513) ,
  expand.grid(
    belt=c("No","Yes"), 
    location=c("Urban","Rural"), 
    gender=c("Female","Male"), 
    injury=c("No","Yes"))
)
#print dataset
obs.frame.accident
```

> Consider the Homogeneity association model $(GZ,GS,GI,ZS,ZI,SI)$,  
> 1. Write your own code to compute the fitted values for $\mu_{i,j,k,c}$  
> 2. Check your ewsults with the R command 'loglin{stats}'


***Solution***

```{r}

# Step 1 :  compute and save the minimal statistics

obs.frame <- obs.frame.accident

obs.xtab <- xtabs(obs.frame)
n_ZG <- margin.table(obs.xtab,c(2,3))
n_SG <- margin.table(obs.xtab,c(1,3))
n_GI <- margin.table(obs.xtab,c(3,4))
n_SZ <- margin.table(obs.xtab,c(1,2))
n_ZI <- margin.table(obs.xtab,c(2,4))
n_SI <- margin.table(obs.xtab,c(1,4))


# Step 2: Create a seed for the fitted mu's

# seed the mu_opt
mu_opt <- obs.xtab
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
      	  mu_opt[s,z,g,i] = 1.0

# Step 3: Perform the loop to approximate the mu's

for (t in 1: 100) {
  
  mu_ZG <- margin.table(mu_opt,c(2,3))
  for(s in 1:2)
    for(z in 1:2)
      for(g in 1:2)
      	for(i in 1:2)
      	  mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_ZG[z,g]/mu_ZG[z,g]

  mu_SG <- margin.table(mu_opt,c(1,3))
  for(s in 1:2)
    for(z in 1:2)
      for(g in 1:2)
      	for(i in 1:2)
      	  mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_SG[s,g]/mu_SG[s,g]
  
  mu_GI <- margin.table(mu_opt,c(3,4))
  for(s in 1:2)
    for(z in 1:2)
      for(g in 1:2)
      	for(i in 1:2)
          mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_GI[g,i]/mu_GI[g,i]
  
  mu_SZ <- margin.table(mu_opt,c(1,2))
  for(s in 1:2)
    for(z in 1:2)
      for(g in 1:2)
      	for(i in 1:2)
          mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_SZ[s,z]/mu_SZ[s,z]
  
  mu_ZI <- margin.table(mu_opt,c(2,4))
  for(s in 1:2)
    for(z in 1:2)
      for(g in 1:2)
      	for(i in 1:2)
          mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_ZI[z,i]/mu_ZI[z,i]
  
  mu_SI <- margin.table(mu_opt,c(1,4))
  for(s in 1:2)
    for(z in 1:2)
      for(g in 1:2)
      	for(i in 1:2)
          mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_SI[s,i]/mu_SI[s,i]
}

mu_opt
```

```{r}
# CHECK WITH COMMAND loglin
obs.frame <- obs.frame.accident
obs.xtab <- xtabs(obs.frame)
loglin(obs.xtab,
         list(
           c(1,2),
           c(1,3),
           c(1,4),
           c(2,3),
           c(2,4),
           c(3,4)
           ),
       fit = TRUE)$fit
```



> Consider the 3 way interaction model $(GLI,GSI,LSI,GLS)$.  
> 1. Write your own code to compute the fitted values for $\mu_{i,j,k,c}$  
> 2. Check your ewsults with the R command 'loglin{stats}'


***Solution***

```{r}

# Step 1 :  compute and save the minimal statistics

obs.frame <- obs.frame.accident

obs.xtab <- xtabs(obs.frame)
n_ZGI <- margin.table(obs.xtab,c(2,3,4))
n_SGI <- margin.table(obs.xtab,c(1,3,4))
n_SZI <- margin.table(obs.xtab,c(1,2,4))
n_SZG <- margin.table(obs.xtab,c(1,2,3))


# Step 2: Create a seed for the fitted mu's

# seed the mu_opt
mu_opt <- obs.xtab
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
      	  mu_opt[s,z,g,i] = 1.0

# Step 3: Perform the loop to approximate the mu's

for (t in 1: 100) {
  
  mu_ZGI <- margin.table(mu_opt,c(2,3,4))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
        mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_ZGI[z,g,i]/mu_ZGI[z,g,i]

  mu_SGI <- margin.table(mu_opt,c(1,3,4))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
        mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_SGI[s,g,i]/mu_SGI[s,g,i]
  
  mu_SZI <- margin.table(mu_opt,c(1,2,4))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
        mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_SZI[s,z,i]/mu_SZI[s,z,i]
  
  mu_SZG <- margin.table(mu_opt,c(1,2,3))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
        mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_SZG[s,z,g]/mu_SZG[s,z,g]
}

mu_opt
```
```{r}
# CHECK WITH COMMAND loglin
obs.frame <- obs.frame.accident
obs.xtab <- xtabs(obs.frame)
loglin(obs.xtab,
         list(
           c(2,3,4),
           c(1,3,4),
           c(1,2,4),
           c(1,2,3)
           ),
       fit = TRUE)$fit
```


> Consider the independent model $(G, S, L, I)$  
> 
> 1. compute the fitted values for $\mu_{i,j,k,c}$, by using your own code  
> 2. check your resutls with  'loglin{stats}'  

***Solution***

```{r}

# Step 1 :  compute and save the minimal statistics

obs.frame <- obs.frame.accident

obs.xtab <- xtabs(obs.frame)
n_S <- margin.table(obs.xtab,c(1))
n_Z <- margin.table(obs.xtab,c(2))
n_G <- margin.table(obs.xtab,c(3))
n_I <- margin.table(obs.xtab,c(4))


# Step 2: Create a seed for the fitted mu's

# seed the mu_opt
mu_opt <- obs.xtab
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
      	  mu_opt[s,z,g,i] = 1.0

# Step 3: Perform the loop to approximate the mu's

for (t in 1: 100) {
  
  mu_S <- margin.table(mu_opt,c(1))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
		mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_S[s]/mu_S[s]
  
  mu_Z <- margin.table(mu_opt,c(2))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
		mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_Z[z]/mu_Z[z]
  
  mu_G <- margin.table(mu_opt,c(3))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
		mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_G[g]/mu_G[g]
  
  mu_I <- margin.table(mu_opt,c(4))
for(s in 1:2)
  for(z in 1:2)
    for(g in 1:2)
    	for(i in 1:2)
		mu_opt[s,z,g,i] <- mu_opt[s,z,g,i]*n_I[i]/mu_I[i]

}


mu_opt
```



```{r}
# CHECK WITH COMMAND loglin
obs.frame <- obs.frame.accident
obs.xtab <- xtabs(obs.frame)
loglin(obs.xtab,
         list(
           c(1),
           c(2),
           c(3),
           c(4)
           ),
       fit = TRUE)$fit
```


---------

# Newton Method 

Newton's method is a general purpose procedure to compute numerically 
the solution of a system of non-linear equations given that a number 
of assumptions are satisfied. 

***In general.***

* Let function $f:\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$. 
* Assume you need to find the solution $x^{*}$ of the equation 

\begin{equation}
f(x^{*})=0\label{eq:asfdgadf}
\end{equation}

* Newtons's method for solving the system (\ref{eq:asfdgadf}) is the
recursion 

\begin{equation}
x^{(t+1)}=x^{(t)}-[\nabla_{x}f(x^{(t)})]^{-1}f(x^{(t)})\label{eq:dfghsdha}
\end{equation}

 for $t\in\mathbb{N}$ and for a pre-specified seed value $x^{(0)}\in\mathbb{R}^{n}$.

+ In theory, Newton's method converges to the solution quadratically;
i.e.
\[
\lim_{t\rightarrow\infty}\frac{|x^{(t+1)}-x^{*}|_{\infty}}{|x^{(t)}-x^{*}|_{\infty}^{2}}=0
\]
under regularity conditions discussed in (Numerical analysis / R.
L. Burden, J. D. Faires.)

* In practice, we run Newton's recursion several times starting from
a different seed each time. 


***An intuitive explanation why it works***

* From the Taylor expansion, and assuming that $\nabla_{x}^{2}f(x)$ is continuous, I get 
\[
f(x_{t+1})=f(x_{t})+\nabla_{x}f(x_{t})(x_{t+1}-x_{t})+O(|x_{t+1}-x_{t}|^{2})
\]
 and by ignoring the error term and rearranging the quantities I get
\[
x_{t+1}\sim x_{t}+\nabla_{x}f(x_{t})(f(x_{t+1})-f(x_{t}))
\]

* If $x_{t+1}$ is the solution, or close to that, then $f(x_{t+1})=0$,
and hence 
\[
x_{t+1}\sim x_{t}-\nabla_{x}f(x_{t})f(x_{t})
\]

* Now, we see that the gradient of $f$ times the values of $f$ at $x_{t}$ leads the sequence towards locations where $f$ is zero. 

* So, eventually, it may work ...


***Pseudo-algorithm of Newton's method:***


`Aim` Approximate the solution of $f(x)=0$

`Input` number of equations $n$; seed $x^{(0)}=(x_{1}^{(0)},...,x_{n}^{(0)})\in\mathbb{R}^{n}$;
tolerance $\tau$ ; maximum number of iterations $T$

`Output:` Approximate solution $x^{*}\in\mathbb{R}^{n}$; trace
of $x^{(t)}$; trace of relative error $\tau^{(t)}=|x^{(t)}-x^{(t-1)}|_{\infty}$;
number of iterations performed $t$ 

1. Set $x_{\text{opt}}=x^{(0)}$
2. Set $t=1$
3. While ($t\le T$) do:
    i) Compute $n\times1$ vector $F\in\mathbb{R}^{n}$ whose
$i$-th element is $F_{i}=f(x_{\text{opt},i})$
    ii) Compute $n\times n$ vector $J\in\mathbb{R}^{n\times n}$
whose $(i,j)$-th element is $J_{i,j}=\frac{\text{d}}{\text{d}x_{j}}f_{i}(x_{\text{opt}})$
for $(i,j)\in\{1,...,n\}^{2}$
    iii) Solve the $n\times n$ linear system $Jy=-F$ and
compute $y\in\mathbb{R}^{n}$
    iv) Update $x_{\text{opt}}=x_{\text{opt}}+y$ 
    v) Compute $\epsilon^{*}= |y|_{\infty}$
    vi) If $\epsilon^{*}<\tau$, then escape from the loop
    vii) Increase the time step $t=t+1$ 
4. Set $x^{*}=x_{\text{opt}}$
5. Return as output: Return as output: $x^{*}$, $t^{*}$, and $\epsilon^{*}$.

### ***Example***

Solve the system of non-linear equations
\[
\begin{cases}
\cos(x_{2}x_{3})+\frac{1}{2} & =3x_{1}\\
81(x_{2}+0.1)^{2} & =x_{1}^{2}+(x_{3}+0.1)^{2}+\sin(x_{3})+1.06\\
-\frac{10\pi-3}{3} & =\exp(-x_{1}x_{2})+20x_{3}
\end{cases}
\]


***Solution***

This is equivalent to solving the system $f(x_{1},x_{2},x_{3})=0$
where
\[
f(x)=\begin{bmatrix}3x_{1}-\cos(x_{2}x_{3})-\frac{1}{2}\\
x_{1}^{2}-81(x_{2}+0.1)^{2}+\sin(x_{3})+1.06\\
\exp(-x_{1}x_{2})+20x_{3}+\frac{10\pi-3}{3}
\end{bmatrix}
\]
 with Jacobian 
\[
\nabla_{x}f(x)=\begin{bmatrix}3 & x_{3}\sin(x_{2}x_{3}) & x_{2}\sin(x_{2}x_{3})\\
2x_{1} & -162(x_{2}+0.1) & \cos(x_{3})\\
-x_{2}\exp(-x_{1}x_{2}) & -x_{1}\exp(-x_{1}x_{2}) & 20
\end{bmatrix}
\]

I need to supply the Newton's algorithm with the quantities above, as well as consider a tolerance,
e.g. $1e-4$ (meaning $10^{-4}$), seed value e.g., $x^{(0)}=(0.1,0.1,-0.1)^{T}$, etc...


> Create a function for $f(x)$, and called `my.f()`


```{r}
my.f <- function(x) {
  f1 <- 3*x[1]-cos(x[2]*x[3])-0.5
  f2 <- x[1]*x[1] -81*(x[2]+0.1)*(x[2]+0.1) +sin(x[3]) +1.06
  f3 <- exp(-x[1]*x[2])+20*x[3]+(10*pi-3)/3
  fvec <- matrix(c(f1,f2,f3),nrow=3)
  return(fvec)
  }
```


> Create a function for $\nabla_x f(x)$, and call it `my.Df`

```{r}
my.Df <- function(x) {
  Df11 <- 3
  Df12 <- x[3]*sin(x[2]*x[3])
  Df13 <- x[2]*sin(x[2]*x[3])
  Df21 <- 2*x[1]
  Df22 <- -162*(x[2]+0.1)
  Df23 <- cos(x[3])
  Df31 <- -x[2]*exp(-x[1]*x[2])
  Df32 <- -x[1]*exp(-x[1]*x[2])
  Df33 <- 20
  fmat <- matrix( c(Df11, Df21, Df31, 
                    Df12, Df22, Df32, 
                    Df13, Df23, Df33 ), 
                  nrow=3, byrow=FALSE)
  return(fmat)
  }
```


> Create a function called `my.newton.method()` which:
> 
> * gets as arguments: 
> 
>     + the function $f(x)$,  
>     + the gradient $\nabla_x f(x)$,  
>     + number of equations $n$;  
>     + seed $x^{(0)}=(x_{1}^{(0)},...,x_{n}^{(0)})\in\mathbb{R}^{n}$;  
>     + tolerance $\tau$ ;   
>     + maximum number of iterations $T$, etc...  
> 
> * returns:  
> 
>     + approximate solution $x^{*}\in\mathbb{R}^{n}$; 
>     + the last relative error $\tau^{*}$;
>     + number of iterations performed $t^{*}$ , etc...
> 
> * use commands 
> 
>     + `solve()` : to solve the system $Ax=b$ 
>     
>     + `while () {...}`: to perform the loop 
>     
>     + `break`: to escape from the loop


```{r}

my.newton.method <- function(my.f, my.Df, x0, tol, Tmax) {
  
  xopt = x0
  
  t = 1
  
  while (t <= Tmax) {
    
    xnow = xopt
    
    F = my.f( xnow )
    
    J = my.Df( xnow )
    
    y = solve(J, -F )
    
    xnew = xnow + y
    
    xopt = xnew
    
    err = max(abs(y))
    
    if ( err <= tol) break
    
    t = t +1 
    
  } 
  
  return(list(xopt=xopt, Tmax=t, err=err))
}

```


> Solve the equation by using the function the you created

```{r}
#Try me ...

x0  = c(0.1,0.1,-0.1)
tol = 0.00001
Tmax = 200

obj <- my.newton.method(my.f, my.Df, x0, tol, Tmax)

obj$xopt
obj$Tmax
obj$err

```




## Newton method for the Log linear model

* I wish to solve non-linear equation $X^{T}n=X^{T}\hat{\mu}(\beta)$
where matrix $X$ is the design matrix given the non-identifiability constrints, e.g., for the model $(XY,XZ,YZ)$. 

* Equivalently, I want to find $\hat{\beta}$ for $f(\hat{\beta})=0$
, where $f(\hat{\beta})=X^{T}(n-\mu(\hat{\beta}))$

* The Jacobian is 
\begin{align*}
\nabla_{\beta}f(\beta) & =\nabla_{\beta}X^{T}(n-\mu(\beta))\;=\nabla_{\beta}[X^{T}\mu(\beta)]\\
 & =X^{T}\text{diag}(\mu(\beta))X
\end{align*}
 Because the $(j,k)$th element of $\nabla_{\beta}[X^{T}\mu(\beta)]$
is 
\begin{align*}
\left[\nabla_{\beta}[X^{T}\mu(\beta)]\right]_{j,k} & =-\frac{\text{d}}{\text{d}\beta_{k}}\sum_{i}X_{i,j}\exp(\sum_{j}X_{i,j}\beta_{j})\\
 & =-\sum_{i}X_{i,j}\exp(\sum_{j}X_{i,j}\beta_{j})X_{i,k}
\end{align*}
since $\mu_{i}(\beta)=\exp(\sum_{j}X_{i,j}\beta_{j})$.

* Then the Newton's recursion (\ref{eq:dfghsdha}) becomes 
\[
\beta_{t+1}=\beta_{t}+[X^{T}\text{diag}(\mu(\beta_{t}))X]^{-1}X^{T}(n-\mu(\beta_{t}))
\]

* It is proven that $\beta_{t}\rightarrow\hat{\beta}$.



## Example (For Homework)

Consider the data-set, Alcohol, Cigarette, and Marijuana

```{r}

## load the data

obs.frame<-data.frame(count=c(911,538,44,456,3,43,2,279),
                      expand.grid( 
  marijuana=factor(c("Yes","No"),levels=c("No","Yes")),
  cigarette=factor(c("Yes","No"),levels=c("No","Yes")),  
  alcohol=factor(c("Yes","No"),levels=c("No","Yes")))
  ) 

## orint the obs.frame

obs.frame

```

> Consider:
> 
> * a Log-linear model describing a homogeneous association between each pair of variables at each level of the third one; i.e. $[AC,AM,CM]$
> 
> * as identifiability constraints the corner points where the reference levels are the last levels; namely, $2$ (yes), $2$ (yes), and $2$ (yes) for marijuana, cigarette, and alcohol.
> 
> Use Newton method in order to compute $\lambda$'s
> 
> Estimate the log linear model coefficients


***Solution ***

```{r}
# This is a homework for further practice.
```





---

# Save me  

Generate the document as a Notebook, PDF, Word, or HTML by choosing the relevant option (from the pop-up menu next to the Preview button). Then save your Markdown code by choosing the relevant option (from the task bar menu).

Save the *.Rmd script, so that you can edit it later.
